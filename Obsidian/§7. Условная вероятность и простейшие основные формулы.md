Мы уже говорили, что в основе определения вероятности события лежит некоторая совокупность условий . Если никаких ограничений, кроме условий $\mathfrak{G}$, при вычислении вероятности $Р(А)$ не налагается, то такие вероятности называются безусловными. 
Однако в ряде случаев приходится рассматривать вероятности событий при дополнительном условии, что произошло некоторое событие $В$. 
Такие вероятности мы будем называть условными и обозначать символом $P(A|B)$: это означает вероятность события $А$ при условии, что событие $В$ произошло. Строго говоря, безусловные вероятности также являются условными, так как исходным моментом построенной теории было предположение о существовании некоторого неизменного комплекса условий $\mathfrak{G}$.
>Пример 1. Брошены две игральные кости. Чему равна вероятность того, что сумма выпавших на них очков равна 8 (событие $А$), если известно, что эта сумма есть четное число (событие В)?
>Все возможные случаи, которые могут представитыя при бросании двух костей, мы запишем в табл. б, каждая клетка которой содержит запись возможного события: на первом месте в скобках указывается число очков, выпавших на первой кости, на втором месте — число очков, выпавших на второй кости.
>![[Pasted image 20230809133631.png]]
>Общее число возможных случаев — 36, благоприятствующих событию $А$ — 5. Таким образом, безусловная вероятность $Р(А) = 5/36$. Если событие $В$ произошло, то осуществилась одна из 18 (а не 36) возможностей и, следовательно, условная вероятность равна $Р(А|В) = 5/18$. 

>Пример 2. Из колоды карт последовательно вынуты две карты. 
>Найти
>	а) безусловную вероятность того, что вторая карта окажется тузом (неизвестно, какая карта была вынута вначале) и
>б) условную вероятность, что вторая карта будет тузом, если первоначально был вынут туз.
>Обозначим через $А$ событие, состоящее в появлении туи на втором месте, а через В — событие, состоящее в появлении туза на первом месте. Ясно, что имеет место равенство
>$$A=A B+A{\bar{B}}$$
>В силу несовместимости событий $АВ$ и $А\overline В$ имеем:
>$\mathrm{P}(A)=\mathrm{P}(A B)+\mathrm{P}\left(A{\overline{{B}}}\right)$
>При вынимании двух карт из колоды в 36 карт могут произойти 36 • 35 (учитывая порядок!) случаев. Из них благоприятствующих событию $АВ$ — 4 • З случая, а событию $АВ$ — 32 • 4 случая. Таким образом, $\mathrm{P}(A)={\frac{4\cdot3}{36\cdot35}}+{\frac{32\cdot4}{36\cdot35}}={\frac{1}{9}}.$
>Если первая карта есть туз, то в колоде осталось 35 карт и среди них только три туза. Следовательно, $\mathsf{P}(A|B)=3/35.$

Общее решение задачи нахождения условной вероятности для классического определения вероятности не представляет труда. В самом деле, пусть из $n$ единственно возможных, несовместимых и равновероятных событий $A_{1}, A_{2},..., A_{n}$
![[Pasted image 20230809134711.png]]
(понятно, что $r\leq k,r\leq m$). Если событие $B$ произошло, то это означает, что наступило одно из событий $A_j$, благоприятствующих $B$. При этом условии событию $A$ благоприятствуют $r$ и только $r$ событий $A_j$, благоприятствующих $AB$. 
Таким образом, ${\mathsf{P}}(A|B)={\frac{r}{k}}={\frac{r/n}{k/n}}={\frac{\mathsf{P}(A B)}{\mathsf{P}(B)}}$(1)
Точно так же, если $P(A)>0$ то ${\mathrm{P}}(B|A)={\frac{\mathrm{P}(A B)}{\mathrm{P}(A)}}$(1')
Понятно, что если $В$ (соответственно $А$) есть невозможное событие,то равенство (1) (соответственно (1') теряет смысл. 
Заметим, что рассуждения, проведенные нами в примерах и 2, не являются доказательствами, а представляют только мотивировки определений, данных равенствами (1) и (1'). 
При Р(А)Р(В) > 0 каждое из равенств (1), эквивалентно так называемой теореме умножения, согласно которой ${\mathsf{P}}(A B)={\mathsf{P}}(A){\mathsf{P}}(B|A)={\mathsf{P}}(B){\mathsf{P}}(A|B)$(2)
т. е. вероятность произведения двух событий равна произведению вероятностей одного из этих событий на условную вероятность Другого при условии, что первое произошло.
Теорема умножения применима и в том случае, когда одно из событий $A$ или $B$ есть невозможное событие, так как в этом случае вместе с $P(A) = 0$ имеют место равенства $P(A|B) = 0$ и $Р(АВ) = 0$
Говорят, что событие $А$ независимо от события $В$, если имеет место
равенство 
${\mathsf{P}}(A|B)={\mathsf{P}}(A)$ (3)
т. е. если наступление события не изменяет вероятности события

Если событие $А$ независимо от $В$, то в силу (2) имеет место равенство
${\mathsf{P}}(A)\operatorname{P}(B|A)=\operatorname{P}(B)\operatorname{P}(A).$Е
Отсюда при $Р(А) > 0$ находим, что 
${\mathsf{P}}(B|A)={\mathsf{P}}(B)$(4)
т. е. событие $В$ также независимо от $А$. Таким образом, свойство незави-
событий взаимно.
Для независимых событий теорема умножения принимает особенно простой вид, а именно, если события $А$ и $В$ независимы, то
$\mathsf{P}(A B)=\mathsf{P}(A)\cdot\mathsf{P}(B)$

Если независимость событий $А$ и $В$ определить посредством равенства
$\mathsf{P}(A B)=\mathsf{P}(A)\mathsf{P}(B),$
то это определение верно всегда, в том числе и тогда, когда $P(A) = 0$ или $P(B)=0$
Понятие независимости событий играет значительную роль в теории вероятностей и в ее приложениях. В частности, большая часть результатов, изложенных в настоящей книге, получена в предположении независимости тех или иных рассматриваемых событий.
В практических вопросах для определения независимости данных событий редко обращаются к проверке выполнения для них равенств (З) и (4). Обычно для этого пользуются интуитивными соображениями, основанными на опыте.

Так, например, ясно, что выпадение герба на одной монете не изменяет появления герба (решки) на другой монете, если только эти моменты во время бросания не связаны между собой (например, жестко не скреплены). Точно так же рождение мальчика у одной матери не изменяет вероятности появления мальчика (девочки) у другой матери. Эти события независимые. Мы обобщим теперь понятие независимости двух событий на совокупность нескольких событий.

События $B_{1},B_{2},..., B_{n}$ называются независимыми в совокупности, если для любого события $B_{p}$ из их числа и произвольных $B_{i_{1}}, B_{i_{2}}...B_{i_r}$ из их же числа и отличных от $B_{p}(i_{n}\neq p\ и\ 1\leq n\leq r)$ события $B_p$ и $B_{i_{1}}, B_{i_{2}}...B_{i_r}$, взаимно независимы.
В силу предыдущего, это определение эквивалентно следующему: 
при любых $1\leqslant i_{1}^{\cdot}\lt i_{2}\lt \dots\lt i_{r}\leqslant s$ и $r\ (1\leqslant r\leqslant s)$.
Заметим, что для независимости в совокупности нескольких событий недостаточно их попарной независимости. 
>Представим себе, что грани тетраэдра — в красный цвет ($А$), 2-я — в зеленый ($В$), 3-я — окрашены: 1-я в синий ($С$) и 4-я — во все эти три цвета ($АВС$). Легко видеть, что вероятность грани, на которую упадет тетраэдр при бросании, в своей
окраске иметь красный цвет равна 1/2: граней четыре и две из них имеют в окраске красный цвет. Таким образом, $\operatorname{P}(A)=1/2.$ 
Точно так же можно посчитать, что
${\mathsf{P}}(B)={\mathsf{P}}(C)={\mathsf{P}}(A|B)={\mathsf{P}}(B|C)={\mathsf{P}}(G|A)={\mathsf{P}}(B|A)=$
$={\mathsf{P}}(G|A)={\mathsf{P}}(C|B)=\mathsf{P}(A|C) = 1/2$.
События $А, В, С$, таким образом, попарно независимы.

Однако, если нам известно, что осуществились события $В$ и $С$, то заведомо осуществилось и событие $А$, т. е. $\mathsf{P(A|B C)=1.}$ 
Таким образом, события $А, В, С$ в совокупности зависимы.
Формула (1'), которая в случае классического определения была нами выведена из определения условной, в случае аксиоматического определения будет взята нами в качестве определения.
Таким образом, в общем случае при $P(A)>0$ по определению $\mathsf{P}(B|A)={\frac{\mathsf{P}(A B)}{\mathsf{P}(A)}}.$
(В случае $Р(А) = 0$ условная вероятность $P(B|A)$ остается неопределенной.) Это позволяет нам перенести автоматически на общее понятие вероятности все определения и результаты настоящего параграфа.
Предположим теперь, что событие $В$ может осуществиться с одним
и только с одним из $n$ несовместимых событий . Иными
словами, положим, что $B=\sum_{i=1}^{n}B A_{i},$
где события $BA_{i}$ и $BA_{ј}$ с разными индексами $i$ и $ј$ несовместимы.
По теореме сложения вероятностей имеем:
${\mathsf{P}}(B)=\sum\limits_{i=1}^{n}{\mathsf{P}}(B A_{i}).$
Использовав теорему умножения, находим, что ${\mathsf{P}}(B)=\sum_{i=1}^{n}{\mathsf{P}}(A_{i}){\mathsf{P}}(B|A_{i}).$
Это равенство носит название формулы полной вероятности и играет основную роль во всей дальнейшей теории.
>В качестве иллюстрации рассмотрим два примера.
>![[Pasted image 20230809170449.png]]![[Pasted image 20230809170517.png]]![[Pasted image 20230809171220.png]]![[Pasted image 20230809171237.png]]

Мы в состоянии теперь вывести важные формулы Байеса или, как иногда говорят, вероятности гипотез. Пусть по-прежнему имеет место равенство (5). Требуется найти вероятность события $A_{i}$, если известно, что $В$ произошло. Согласно теореме умножения имеем:
$${\mathsf{P}}(A_{i}B)={\mathsf{P}}(B){\mathsf{P}}(A_{i}|B)={\mathsf{P}}(A_{i}){\mathsf{P}}(B|A_{i}).$$
Отсюда:
$${\mathsf{P}}(A_{i}|B)={\frac{{\mathsf{P}}(A_{i}){\mathsf{P}}(B|A_{i})}{{\mathsf{P}}(B)}},$$
используя формулу полной вероятности, находим, что
$\mathrm{P}(A_{i}|B)={\frac{\mathrm{P}(A_{i})\mathrm{P}(B|A_{i})}{\displaystyle\sum\limits^{n}_{j=1}\mathrm{P}(A_{j})\mathrm{P}(B|A_{j})}}.$

Полученные нами формулы название формул Байеса. Общая схема применения этих фюрмул к решению практических задач такова. 
Пусть событие В может протекать в различных условиях, относительно характера которых может быть сделано $n$ гипотез: $A_{1}, A_{2},..., A_{n}$.
По тем или иным причинам нам известны вероятности $P(A_{i})$ этих гипотез до испытания. Известно также, что гипотеза $A_{i}$ сообщает событию $В$ вероятность $P(B|A_{i})$. Произведен опыт, в котором событие $В$ наступило. 
Это должно вызвать переоценку вероятностей гипотез $A_{i}$ — формулы Байеса количественно решают этот вопрос.
В артиллерийской практике производится так называемая пристрелка, имеющая своей целью уточнить наши знания относительно условий стрьбы (например, правильность прицела). В теории пристрелки широко используется формула Байеса. 
>Мы ограничимся приведением чисто схематического примера исключительно ради иллюстрации характера задач, решаемых этой формулой.
>![[Pasted image 20230809172049.png]]![[Pasted image 20230809172057.png]]